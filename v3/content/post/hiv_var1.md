---
title: "Viral variation: a Galaxy tutorial"
date: 2019-10-29
author: Anton
tags: [Galaxy,RNA viruses,HIV]
show_summary: false 
math: true
---

<div class="alert alert-danger" role="alert">
This tutorials will soon be converted into an official <a href="https://training.galaxyproject.org/">Galaxy Training Material</a>
</div>

## Questions

- How to identify variants in viral genomes?
- How to separate reliable calls from spurious?
- How to analyze dozens of samples simultaneously?

## Objectives

- Highlight analysis of real life data containing many samples
- Demonstrate dynamic downsampling using Galaxy's Expression Tools
- Show how to annotate drug resistance mutations
- Highlight the importance of Jupyter notebooks

## Key points

- Real datasets contain many samples - Galaxy can handle this with ease
- High coverage in viral re-sequencing data may cause problems
- Strand bias estimation can help to eliminate spurious calls
contributors

---


# Introduction

Identifying sequence variants in pathogens such as viruses and bacteria is important for obvious reasons. It facilitates understanding of infectious agent evolution, localization of mutations conferring drug resistance, identification of founding populations, tracking infection spread and so on. 

In this tutorial we will use datasets generated by [Jair et al. 2019](http://dx.doi.org/10.1371/journal.pone.0214820). It is a re-sequencing dataset in which the authors amplified several segments of the *pol* gene (for HIV-I genome structure see Fig. [1](#figure-1) below) and sequenced resulting amplicons from 68 individuals. 

------

##### Figure 1
Landmarks of the HIV-1 genome, HXB2 (from <a href='https://https://www.hiv.lanl.gov/content/sequence/HIV/MAP/landmark.html'>LANL HIV Genome Database</a>)

![HIV genome map](https://www.hiv.lanl.gov/content/sequence/HIV/IMAGES/hxb2genome.gif)

------

Our main goal will be to analyze multiple datasets simultaneously to identify variants corresponding to known drug resistance mutations as well as potential new genome alterations.

# Loading multiple datasets from the web

## Datasets

For this tutorial we downsampled all datasets produced by [Jair et al. 2019](http://dx.doi.org/10.1371/journal.pone.0214820) ([PRJNA517147](https://www.ncbi.nlm.nih.gov/bioproject/?term=PRJNA517147)) to approximately 10% of the original size. This was done to ensure that analyses can be performed quickly. The downsampled datasets are available from Zenodo [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.3519268.svg)](https://doi.org/10.5281/zenodo.3519268). 

<!-- **PAIRED END DATA EXPLANATION**  -->

<div class="alert alert-warning" role="alert">
Here we based all examples of just four datasets listed below. We recommend starting with these. You can of course analyze all 68, but starts with these four to get the feel for how things work.
</div>

```
SRR8525909
SRR8525905
SRR8525896
SRR8525902
```

## Loading datasets using rule builder

Above, we listed the four datasets we are going to play with. But these are just IDs. In order to download actual data (fastq files) associated with these we need to construct URLs to let Galaxy know where to download data from. 

The full URL can be constructed by adding the following suffix:

```url
https://zenodo.org/record/3519268/files/
```

and the following prefix:

```
.fq.gz
```

so that the full URL looks something like this:

```
https://zenodo.org/record/3519268/files/SRR8525909.fq.gz
```

But we need to do this for all four datasets (or all 68, or a few thousand if you analyze other data). To help with this we use **Rule builder** - a part of Galaxy's dataset upload functionality.

<!--- VIDEO GOES HERE --->

## Upload with Rule Builder 

 1. Click the upload icon {% icon galaxy-upload %} in the upper left corner of the Galaxy interface
 2. Select **Rule-based** tab
 3. In **Upload data as** dropdown select *Collection(s)*
 4. Copy and paste the four dataset IDs listed above into the large text box
 5. Click **Build** button
 6. Follow step from the video to create a collection

Alternatively you can import the following JSON block into rule builer:

<p>
  <a class="btn btn-primary" data-toggle="collapse" href="#collapseExample" role="button" aria-expanded="false" aria-controls="collapseExample">
    JSON blob
  </a>
<div class="collapse" id="collapseExample">
  <div class="card card-body">
  	<p>
  	<tt>
    {
  "rules": [
    {
      "type": "add_column_value",
      "value": "https://zenodo.org/record/3519268/files/"
    },
    {
      "type": "add_column_concatenate",
      "target_column_0": 1,
      "target_column_1": 0
    },
    {
      "type": "remove_columns",
      "target_columns": [
        1
      ]
    },
    {
      "type": "add_column_value",
      "value": ".fq.gz"
    },
    {
      "type": "add_column_concatenate",
      "target_column_0": 1,
      "target_column_1": 2
    },
    {
      "type": "remove_columns",
      "target_columns": [
        1,
        2
      ]
    }
  ],
  "mapping": [
    {
      "type": "list_identifiers",
      "columns": [
        0
      ],
      "editing": false
    },
    {
      "type": "url",
      "columns": [
        1
      ]
    }
  ],
  "extension": "fastqsanger.gz"
}
</tt>
  </div>
</div>


Once the collection is created and uploaded, you will see a new green box in the history pane as shown in Fig. [2](#figure-2) below:

<hr>

##### Figure 2
HIV datasets uploaded as a collection. In this case the collection contains four datasets

![](/lab_site/images/hiv_collection.png)


<hr>

# Quality control

## Assessing read quality

Once the data is we will need to assess its quality.

#### Run **FastQC** with the following parameters:

 - *"Short read data from your current history"*: `Jair data` (or whatever other name you gave that collection; red arrow in Fig. [3](#figure-3))
 - Click **Execute** button

<div class="alert alert-warning" role="alert">
Note that <b>FastQC</b> produces two output collections: (1) "Webpage" and (2) "Raw Data".
</div>

<hr>

##### Figure 3
FastqQC using collection as an input. Note that a collections button  was pressed to reveal the collection in the history

![Fastqc interface](/lab_site/images/hiv_fastqc1.png)

<hr>

**FastQC** produces an individual report for each dataset. This means that in order to get an idea about quality of the data one needs to click report corresponding to each element of the collection. This may be possible for four datasets but will be very difficult for more than that. Fortunately, there is a tool, **MultiQC** that summarize data for all datasets at once. It takes collection produced by **FastQC** as input and produces a single report. Let apply it our example:

#### Summarizing FastQC results

#### Run **MultiQC** with the following parameters:

 - *"Which tool was used generate logs?"* : `FastQC` (red arrow in  Fig. [4](#figure-4))
 - *"FastQC output"*: `FastQC on collection 1: Raw data`; blue arrow in Fig. [4](#figure-4))
 - Click **Execute** button

<hr>

##### Figure 4
Running MultiQC on FastQC output. Because FastQC was run on a collection, it produced collections as outputs. Here one of these collections, 'Raw data', is used as an input

![MultiQC interface](/lab_site/images/hiv_multiqc1.png)

<hr>

MultiQC produces a variety of graphical summaries including distribution of quality values across reads shown in Fig. 5:

##### Figure 5
Distribution of quality values across four datasets in our example.

![MultiQC report](/lab_site/images/hiv_multiqc2.png)

<hr> 

## Trimming adapters

The original data produced by {% cite Jair2019-hw %} was contaminated with [Nextera adapter sequences](https://www.nature.com/protocolexchange/system/uploads/6661/original/SupplementaryDocument2-illumina-adapter-sequences-Feb2018.pdf?1530635414). We removed these adapters during downsampling (because they interfered with mapping). However, for the same of this presentation, let assume that our reads are in fact contaminated with adapters. 

A great tool for automatic detection and removal of adapters in **Trim Galore!**, which, in turn, is based on another widely used tools called **CutAdapt**. Let use **trim-galore!** to remove adapters:

#### Trimming adapters

Run **Trim Galore!** with the following parameters:

 - *"Is this library paired- or single-end?"* : `Single-end` (green arrow in Fig. [6](#figure-6))
 - *"Reads in FASTQ format"* : `Jair data` (or whatever other name you gave that collection; grey arrow in Fig. [6](#figure-6)
 - *"Trim Galore! advanced settings"* : `Full parameter list` (blue arrow in Fig. [6](#figure-6))
 - *"Discard reads that became shorter than length N"* : `0` (red arrow in Fig. [6](#figure-6))
 - Click **Execute** button

<div class="alert alert-warning" role="alert">
<b>Why do we use `Single-end` setting while our data is actually paired-end?</b><br><hr> Our data is in interleaved paired-end format. It is a single file that contains forward and reverse reads in an alternating order. Here we are fooling <b>Trim Galore!</b> into processing this file as a single end. In order to do this we set <em>"Discard reads that became shorter than length N"</em> to <tt>0</tt>. This guarantees that no reads will be thrown away and the alternating order of forward and reverse reads in our file will not be disrupted. 
</div>

<div class="alert alert-info" role="alert">

<b>Tip: What did Trim Galore! do?</b><hr>
If you scroll down interface of the <b>Trim Galore!</b> tool, you will see <em>"Generate a report file"</em> selector. Setting this to <tt>Yes</tt> will produce a report dataset that can be processed with <b>MultiQC</b>. This will provide information on what types and how many adapters have been removed from the reads. When running <b>MultiQC</b> simply set <em>"Which tool was used generate logs?"</em> to <tt>CutAdapt / Trim Galore!</tt>.
</div>

<hr>

##### Figure 6
Trim Galore! interface. Note that <em>Discard reads that became shorter than length N</em> is set to <tt>0</tt>(red arrow)

![Trim Galore interface](/lab_site/images/hiv_tg1.png)

<hr>

# Mapping and estimating coverage

## Get the HIV genome 

Now it is time to map the reads against the HIV reference. We need to download that reference first. The sequence can be obtained directly from [NCBI](https://www.ncbi.nlm.nih.gov/) using the following URL:

```
https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=nucleotide&id=K03455.1&rettype=fasta
```

This URL can be pasted directly into **Upload** utility:

#### Upload with Upload Utility 

1. Click the upload icon {% icon galaxy-upload %} in the upper left corner of the Galaxy interface
2. Select **Regular** tab
3. Click **Paste/Fetch data** button
4. A text box *"Download data from the web by entering URLs (one per line) or directly paste content"* will appear
5. Paste the URL we just mentioned into this box
6. Set **Type** to `fasta`
7. Click **Start**
8. A dataset with a long ugly name will appear in history
9. Rename dataset as `hxb2` (click the pencil icon)

## Map against HIV genome using **BWA-MEM**

Next, we will map reads trimmed at the previous step against the newly downloaded genome, which just appeared as a new history item:

#### Mapping with BWA-MEM

Use **BWA-MEM** with the following parameters:

 1. *"Will you select a reference genome from your history or use a built-in index?"* : `Use a genome from history and build index` (red arrow in Fig. [7](#figure-7))
 2. *"Use the following dataset as the reference sequence"* : `hxb2` or however you named just uploaded HIV genome (blue arrow in Fig. [7](#figure-7))
 3. *"Single or Paired-end reads"* : `Paired Interleaved` (green arrow in Fig. [7](#figure-7))
 4. *"Select fastq dataset"* : Click {% icon param-collection %} and select collection produced by **Trim Galore!** (orange arrow in Fig. [7](#figure-7))
 5. *"Set read groups information?"* : `Set read groups (SAM/BAM specification)`
 6. Click **Execute**

<hr>

##### Figure 7
**BWA MEM** interface

![bwa mem interface](/lab_site/images/hiv_bwa1.png)

</hr>

## Estimate coverage

To proceed with variant calling we need to estimate coverage or reads across the HIV genome. The first step in this analysis is to compute coverage using **bedtools Genome Coverage** tool:

#### Calculating coverage with BEDtools

Use **bedtools Genome Coverage** with the following parameters:

 1. *"Input type"* : Set to `BAM` (red arrow in Fig. [8](#figure-8))
 2. *"BAM file"* : Click {% icon param-collection %} and select collection produced by **BWA MEM** during the previous step (blue arrow in Fig. [8](#figure-8))
 3. Click **Execute**

<hr>
![bamtools genome coverage interface](/lab_site/images/hiv_cvrg1.png)
##### Figure 8
**Bedtools Genome Coverage** interface.
<hr>

**bedtools Genome Coverage** will produce output that would look like this

```
K03455.1	2016	2017	185
K03455.1	2017	2019	187
K03455.1	2019	2020	188
K03455.1	2020	2022	186
K03455.1	2022	2023	187
K03455.1	2023	2026	188
```

where the first column is the genome id, second and third are start and end of a bin, respectively, and the last one is coverage. Such data is produced for all four datasets in our collection. We need to aggregate these data by computing mean and median across the four datasets. This can be done with **Datamash** tool:

#### Computing mean and median with Datamash

Use **Datamash** with the following parameters:

 1. *"Input tabular dataset"* : Click and select collection produced by **bedtools Genome Coverage** during the previous step (red arrow in Fig. [9](#figure-9))
 2. *"Operation to perform on each group"* : set **Type** to `mean` and **On column** to `4` (blue arrow in Fig. [9](#figure-9))
 3. Click *"Insert operation to perform on each group*" (orange arrow in Fig. [9](#figure-9))
 4. Repeat step 2 with one exception: set **Type** to `median` (green arrow in Fig. [9](#figure-9))
 5. Click **Execute**

<hr>

##### Figure 9
**Datamash** interface

![datamash interface](/lab_site/images/hiv_datamash1.png)

<hr>

for each dataset **Datamash** will produce two numbers like this:

```
918.05347593583	880
```

where the first value is mean and the last is median.

## Collapsing collection into a single dataset

Now we want to aggregate these into a single dataset and generate a histogram. To do this we will use a special class of Galaxy tools called collection operations. Specifically, we will use **Collapse Collection** tool:

#### Collapsing a dataset collection

Use **Collapse Collection** with the following parameters:

 1. *"Collection of files to collapse into single dataset"* : Click collection button (folder icon) and select collection produced by **Datamash** during the previous step (red arrow in Fig. [10](#figure-10))
 2. *"Prepend File name"* :`Yes` (blue arrow in Fig. [10](#figure-10))
 3. *"Where to add dataset name*" : `Same line and each line in dataset` (green arrow in Fig. [10](#figure-10))

<hr>

##### Figure 10
Collapse collection interface

![collapse collection](/lab_site/images/hiv_collapse1.png)

<hr>

**Collapse Collection** will produce the following output:

```
SRR8525909	918.05347593583	880
SRR8525905	269.6483909416	274
SRR8525896	85.444444444444	4
SRR8525889	11922.120564344	12696
```

you can see that selecting options highlighted with blue and green arrows (see Fig. 10) instructed **Collapse Collection** to pre-pend dataset names to each corresponding line. We can now use these data to draw a simple barplot using Galaxy's built-in visualization engine:

#### Building a histogram
 1. Locate **Visualize** at the very top of Galaxy interface
 2. Click and select **Create visualization**
 3. Click on **Bar Horizontal (NVD3)**
 4. A dropdown **Select a dataset to visualize:** will appear. From this dropdown select the result of **Collapse Collection** step (red arrow in Fig. [11](#figure-11))
 5. A bar chart will appear. Use disk icon to modify chart parameters. 
 6. Change **Data point labels** to `Column 1`
 7. Change **Values for x-axis** to `Column 1`

<hr>

##### Figure 11
Selecting dataset to produce bar chart.

![Selecting dataset for bar chart](/lab_site/images/hiv_chart1.png)

<hr>

The resulting chart will look like this:

<hr>

##### Figure 12
Selecting dataset to produce bar chart.

![Selecting dataset for bar chart](/lab_site/images/hiv_chart2.png)

<hr>

Here you can see that coverage varies dramatically between datasets.

# Dynamic downsampling

The chart above shown that one of the datasets, `SRR8525889`, has a very high coverage. Normally this should not be a problem. However, because we will performing variant calling using **Freebayes** this may cause problems. Specifically, **Freebayes** tends to "stall" on the datasets that have high coverage variation - it never finished. To alleviate this problem we can simply downsample the BAM files to approximately 1,000&#xd7; coverage. 

Yet there is one difficulty. If we were dealing with a single dataset, that would be easy - just run a downsampling tools (such as **DownsampleSam** from the Picard package). But we have multiple datasets. These datasets also have diffent coverage, so each needs to be downsampled differently. Thus we need some way for doing this automatically (just imagine having hundreds or trousands of datasets). 

Galaxy has a new functionality for addressing problems of this kind. This functionality is called *expression tools*. (To learn more about expression tools see the following [tutorial](https://training.galaxyproject.org/training-material/topics/galaxy-ui/tutorials/workflow-parameters/tutorial.html)).

The basic idea is this:

 - we compute the mean coverage for each dataset
 - we feed this value to a special **expression tool**
 - this **expression tool** puts this value (or *expression*) as a **parameter** to downsampling tool
 - because this value will be calculated for each dataset individually, they will be downsampled dynamically: each according to its own coverage mean.

<div class="alert alert-info" role="alert">
<b>The point of expression tools</b><hr>
Normal Galaxy tools take files as inputs and produce files as outputs. The expression tools take files as inputs but instead of producing files as outputs they output parameters that can be used in other tools.
</div>

Let's do this by example. Look at the following workflow in Fig. 13:

<hr>

##### Figure 13
Downsampling workflow from 10,000 feet.

![Downsampling workflow](/lab_site/images/hiv_ds_workflow1.png)

<hr>

It has takes two inputs ("BAMs" and "Datamash results") and four steps. Before discussing this workflow in details let's describe the overall logic.

## The logic

For dataset `SRR8525902` mean and median of coverage look like this:

```
8407.4972138877	7140
```

To downsample a dataset with mean coverage of $$8407$$ to about $$1000\times$$ we need to downsample it by:

$$ \frac{1000}{8407} \approx 0.11 $$

the means that we need to keep approximately 11% of the original reads. At the same time other datasets in our example have relatively low coverage. For example `SRR8525905` has the following stats:

```
269.6483909416	274
```

for this dataset we do not need to do anything. However, because we will be computing expression

$$ \frac{1000}{\bar{C}} $$

(where C is the mean coverage) for all samples for `SRR8525905` we will get ~ 3.7, which is larger than 1 and therefore impossible. To avoid such situations we will be taking a $$min(\frac{1000}{\bar{C}},1)$$.


## Workflow Inputs

1. The first input in BAM datasets. These, in the case of our tutorial, are generated with **BWA-MEM**. It is just a collection of BAM datasets. It is exactly what we've produced at [mapping step](#map-against-hiv-genome-using-bwa-mem).
2. The second input is **Datamash** output produced by us at [estimate coverage](#estimate-coverage) step. 

## Workflow Steps

### Compute

The first step, `Compute` takes input of **Datamash**. Again, it looks like this:

```
269.6483909416	274
```

The **Compute** tool is designed to perform simple computations on tab delimited files. In this case we will set *"Add expression"* parameter if this tool to `min(1000/c1,1)`. This means that the tools will compute the value of 1,000 divided by the content of the first column `c1`. If this value is less than 1, it will add a column to the input dataset containing this value. If the values is greater than 1, it will add `1` as a new column to the input dataset. The following table contains an example of such calculation for two datasets:

|                  |  SRR8525905 | SRR8525889 |
|------------------|:-------------:|:------------:|
| Input to **Compute** tool       | `269.64 274` | `11922.12 12696` |
| Output of **Compute** tool       | `269.64 274 1` | `11922.12 12696 0.083` |

### Cut

The step cuts out the column produced by **Compute** tool from its output. This step is executed using **Cut** tools. To illustrate what is happening let's use the table from previous step:

|                  |  SRR8525905 | SRR8525889 |
|------------------|:-------------:|:------------:|
| Input to **Cut** tool       | `269.64 274 1` | `11922.12 12696 0.083` |
| Output of **Cut** tool       | `1` | `0.083` |

### Parse parameter value

This is the **Expression tool** step. In Fig. [13](#figure-13) you can see that the *"Float param"* output of **Parse parameter value** expression tool is connected with *"Probability"* parameter of **Downsample SAM/BAM** tool. Thus it supplies this tool with the *"Probability"* value appropriate for a given dataset. Thus this tools get different probability for each input dataset. 

### Downsample SAM/BAM

This step randomly samples reads from the BAM value according to supplied *"Probability"* value. If it is, for example, `0.083` it will sample approximately 8% of the reads. If it is `1` is will sample all the reads.


## Recreating workflow

To run the workflow you can either construct it from scratch or import prebuilt copy from the following URL:

```
https://workshop.usegalaxy.org/u/anton/w/downsample
```

Let's create workflow from scratch as shown here:

<hr>

##### Figure 15
Creating downsampling workflow with expression tools. It extremely important to save your workflow after you have built it by clicking on the save (disk) icon. 

<hr>

![Downsampling workflow](/lab_site/images/create_ds_workflow.gif)

## Running workflow

Once the workflow is created we can run it on the collection that was produced by the running **BWA-MEM** on HIV-1 genome. 

#### Running downsampling workflow
 1. Locate **Workflow** at the very top of Galaxy interface
 2. Select the workflow you've just created (it is named `ds` in Fig. [14](#figure-14), but you could have named it any way you want). 
 3. Click on the down arrow adjacent to the workflow name and select **Run**.
 4. In the interface that would appear you need to change three things:
   - *"BAMs"* : set to the output of **BWA-MEM** from the [mapping step](#map-against-hiv-genome-using-bwa-mem) (red arrow in Fig. [15](#figure-15))
   - *"Datamash"* : set to the <a href="#computing-mean-and-median-with-datamash">output of **Datamash**</a> (blue arrow in Fig. [15](#figure-15))
   - *"Add expression"* : set to `min(1000/c1,1)`. This expression will output minimal of two values: `1000/c1` or `1` (orange arrow in Fig. [15](#figure-15))
 5. Click **Run workflow**

<hr>

##### Figure 15
Running downsampling workflow

![Running downsampling workflow](/lab_site/images/hiv_run_wf.png)

<hr>

# Calling variants

We are now ready to identify sequence variants in our data. For this purpose we will use **Freebayes** - a versatile variant calling suitable for mixed haploid samples. 

#### Calling variants with Freebayes

 1. *"Choose the source for the reference genome"* : `History` (red arrow in Fig. [16](#figure-16))
 2. *"BAM dataset"* : Set this to the collection produced by the downsampling workflow (blue arrow in Fig. [16](#figure-16))
 3. *"Use the following dataset as the reference sequence"* : Set this to HIV genome sequence [uploaded previously](#get-the-hiv-genome) (orange arrow in Fig. [16](#figure-16))
 4. *"Choose parameter selection level"* : `Full list of options`
 5. Locate *"Population model options"* dropdown and set it to `Set population model options`. Within this section set the following:
    - *"Set ploidy for the analysis"* : `1` (red arrow in Fig. [17](#figure-17))
    - *"Output all alleles which pass input filters, regardless of genotyping outcome or model"* : `Yes` (blue arrow in Fig. [17](#figure-17))
 6. Locate *"Input filters"* dropdown and set it to `Set input filters`. Within this section set the following:
    - *"Use stringent input base and mapping quality filters"* : `Yes` (red arrow in Fig. [18](#figure-18))
    - *"Require at least this fraction of observations supporting an alternate allele within a single individual in the in order to evaluate the position"* : `0.001` (blue arrow in Fig. [18](#figure-18))
    - *"Require at least this count of observations supporting an alternate allele within a single individual in order to evaluate the position"* : `10` (orange arrow in Fig. [18](#figure-18))
 7. Click **Execute**
<hr>

##### Figure 16
Setting inputs for Freebayes

![Freebayes datasets](/lab_site/images/hiv_freebayes1.png) 

<hr>

##### Figure 17
Setting population options

![Freebayes population settings](/lab_site/images/hiv_freebayes2.png)

<hr>

##### Figure 18
Setting input filters

![Freebayes input filters](/lab_site/images/hiv_freebayes3.png)

<hr>

# MORE TOMORROW!
