<!doctype html>

<html lang="en" class="h-100">
  <head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <script type='text/javascript' src='https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js'></script>
  <meta name="generator" content="Hugo 0.58.3" />
  <link rel="stylesheet" href="https://nekrut.github.io/lab_site/css/bootstrap.min.css">
  
  
  <title>Viral variation: a Galaxy tutorial | AN LAB</title>
  <style>
.container {
  max-width: 700px;
}
#nav a {
  font-weight: bold;
  color: inherit;
}
#nav a.nav-link-active {
  background-color: #212529;
  color: #fff;
}
#nav-border {
  border-bottom: 1px solid #212529;
}
#main {
  margin-top: 1em;
  margin-bottom: 4em;
}
#home-jumbotron {
  background-color: inherit;
}
#footer .container {
  padding: 1em 0;
}
#footer a {
  color: inherit;
  text-decoration: underline;
}
.font-125 {
  font-size: 125%;
}
.tag-btn {
  margin-bottom: 0.3em;
}
pre {
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
  padding: 16px;
}
pre code {
  padding: 0;
  font-size: inherit;
  color: inherit; 
  background-color: transparent;
  border-radius: 0;
}
code {
  padding: 2px 4px;
  font-size: 90%;
  color: #c7254e;
  background-color: #f9f2f4;
  border-radius: 4px;
}
img,
iframe,
embed,
video,
audio {
  max-width: 100%;
}
.card-img,
.card-img-top,
.card-img-bottom {
  width: initial;
}
</style>
</head>
  <body class="d-flex flex-column h-100">
    <div id="nav-border" class="container">
  <nav id="nav" class="nav justify-content-center">
  
  
  
    
    
      
      
      
      
        
      
    
    
    <a class="nav-link " href="/lab_site/"><i data-feather="home"></i> Home</a>
  
    
    
      
      
      
      
        
      
    
    
    <a class="nav-link " href="/lab_site/people/"><i data-feather="smile"></i> People</a>
  
    
    
      
      
      
      
        
      
    
    
    <a class="nav-link " href="/lab_site/pubs/"><i data-feather="book-open"></i> Pubs</a>
  
    
    
      
      
      
      
       
        
        
      
    
    
    <a class="nav-link nav-link-active" href="/lab_site/post/"><i data-feather="file-text"></i> Blog</a>
  
    
    
      
      
      
      
        
      
    
    
    <a class="nav-link " href="/lab_site/coord/"><i data-feather="map-pin"></i> Coord</a>
  
    
    
      
      
      
      
        
      
    
    
    <a class="nav-link " href="http://galaxyproject.org"><i data-feather="arrow-up"></i> Galaxy</a>
  
  </nav>
</div>
    <div class="container">
      <main id="main">
        

<h1>Viral variation: a Galaxy tutorial</h1>


<i data-feather="calendar"></i> <time datetime="2019-10-29">Oct 29, 2019</time>

  <br>
  <i data-feather="tag"></i>
  
  
  <a class="btn btn-sm btn-outline-dark tag-btn" href="https://nekrut.github.io/lab_site/tags/galaxy">Galaxy</a>
  
  
  <a class="btn btn-sm btn-outline-dark tag-btn" href="https://nekrut.github.io/lab_site/tags/rna-viruses">RNA viruses</a>
  
  
  <a class="btn btn-sm btn-outline-dark tag-btn" href="https://nekrut.github.io/lab_site/tags/hiv">HIV</a>
  

<br><br>


<div class="alert alert-danger" role="alert">
This tutorials will soon be converted into an official <a href="https://training.galaxyproject.org/">Galaxy Training Material</a>
</div>

<h2 id="questions">Questions</h2>

<ul>
<li>How to identify variants in viral genomes?</li>
<li>How to separate reliable calls from spurious?</li>
<li>How to analyze dozens of samples simultaneously?</li>
</ul>

<h2 id="objectives">Objectives</h2>

<ul>
<li>Highlight analysis of real life data containing many samples</li>
<li>Demonstrate dynamic downsampling using Galaxy&rsquo;s Expression Tools</li>
<li>Show how to annotate drug resistance mutations</li>
<li>Highlight the importance of Jupyter notebooks</li>
</ul>

<h2 id="key-points">Key points</h2>

<ul>
<li>Real datasets contain many samples - Galaxy can handle this with ease</li>
<li>High coverage in viral re-sequencing data may cause problems</li>
<li>Strand bias estimation can help to eliminate spurious calls
contributors:</li>
</ul>

<hr />

<h1 id="introduction">Introduction</h1>

<p>Identifying sequence variants in pathogens such as viruses and bacteria is important for obvious reasons. It facilitates understanding of infectious agent evolution, localization of mutations conferring drug resistance, identification of founding populations, tracking infection spread and so on.</p>

<p>In this tutorial we will use datasets generated by {% cite Jair2019-hw %}. It is a re-sequencing dataset in which the authors amplified several segments of the <em>pol</em> gene (for HIV-I genome structure see Fig. <a href="#figure-1">1</a> below) and sequenced resulting amplicons from 68 individuals.</p>

<hr />

<p><img src="https://www.hiv.lanl.gov/content/sequence/HIV/IMAGES/hxb2genome.gif" alt="HIV genome map" /></p>

<h5 id="figure-1">Figure 1</h5>

<p>Landmarks of the HIV-1 genome, HXB2 (from <a href='https://https://www.hiv.lanl.gov/content/sequence/HIV/MAP/landmark.html'>LANL HIV Genome Database</a>)</p>

<hr />

<p>Our main goal will be to analyze multiple datasets simultaneously to identify variants corresponding to known drug resistance mutations as well as potential new genome alterations.</p>

<h1 id="loading-multiple-datasets-from-the-web">Loading multiple datasets from the web</h1>

<h2 id="datasets">Datasets</h2>

<p>For this tutorial we downsampled all datasets produced by <a href="http://dx.doi.org/10.1371/journal.pone.0214820">Jair et al. 2019</a> (<a href="https://www.ncbi.nlm.nih.gov/bioproject/?term=PRJNA517147">PRJNA517147</a>) to approximately 10% of the original size. This was done to ensure that analyses can be performed quickly. The downsampled datasets are available from Zenodo <a href="https://doi.org/10.5281/zenodo.3519268"><img src="https://zenodo.org/badge/DOI/10.5281/zenodo.3519268.svg" alt="DOI" /></a>.</p>

<!-- **PAIRED END DATA EXPLANATION**  -->

<div class="alert alert-warning" role="alert">
Here we based all examples of just four datasets listed below. We recommend starting with these. You can of course analyze all 68, but starts with these four to get the feel for how things work.
</div>

<pre><code>SRR8525909
SRR8525905
SRR8525896
SRR8525902
</code></pre>

<h2 id="loading-datasets-using-rule-builder">Loading datasets using rule builder</h2>

<p>Above, we listed the four datasets we are going to play with. But these are just IDs. In order to download actual data (fastq files) associated with these we need to construct URLs to let Galaxy know where to download data from.</p>

<p>The full URL can be constructed by adding the following suffix:</p>

<pre><code class="language-url">https://zenodo.org/record/3519268/files/
</code></pre>

<p>and the following prefix:</p>

<pre><code>.fq.gz
</code></pre>

<p>so that the full URL looks something like this:</p>

<pre><code>https://zenodo.org/record/3519268/files/SRR8525909.fq.gz
</code></pre>

<p>But we need to do this for all four datasets (or all 68, or a few thousand if you analyze other data). To help with this we use <strong>Rule builder</strong> - a part of Galaxy&rsquo;s dataset upload functionality.</p>

<!-- To understand how Rule Builder operates it is best to watch the following video

```
VIDEO PLACEHOLDER
```
-->

<p>Now that you&rsquo;ve watched the video, do it yourself:  &ndash;&gt;</p>

<h2 id="upload-with-rule-builder">Upload with Rule Builder</h2>

<ol>
<li>Click the upload icon {% icon galaxy-upload %} in the upper left corner of the Galaxy interface</li>
<li>Select <strong>Rule-based</strong> tab</li>
<li>In <strong>Upload data as</strong> dropdown select <em>Collection(s)</em></li>
<li>Copy and paste the four dataset IDs listed above into the large text box</li>
<li>Click <strong>Build</strong> button</li>
<li>Follow step from the video to create a collection</li>
</ol>

<p>Alternatively you can import the following JSON block into rule builer:</p>

<p><p>
  <a class="btn btn-primary" data-toggle="collapse" href="#collapseExample" role="button" aria-expanded="false" aria-controls="collapseExample">
    JSON blob
  </a>
<div class="collapse" id="collapseExample">
  <div class="card card-body">
    <p>
    <tt>
    {
  &ldquo;rules&rdquo;: [
    {
      &ldquo;type&rdquo;: &ldquo;add_column_value&rdquo;,
      &ldquo;value&rdquo;: &ldquo;<a href="https://zenodo.org/record/3519268/files/&quot;">https://zenodo.org/record/3519268/files/&quot;</a>
    },
    {
      &ldquo;type&rdquo;: &ldquo;add_column_concatenate&rdquo;,
      &ldquo;target_column_0&rdquo;: 1,
      &ldquo;target_column_1&rdquo;: 0
    },
    {
      &ldquo;type&rdquo;: &ldquo;remove_columns&rdquo;,
      &ldquo;target_columns&rdquo;: [
        1
      ]
    },
    {
      &ldquo;type&rdquo;: &ldquo;add_column_value&rdquo;,
      &ldquo;value&rdquo;: &ldquo;.fq.gz&rdquo;
    },
    {
      &ldquo;type&rdquo;: &ldquo;add_column_concatenate&rdquo;,
      &ldquo;target_column_0&rdquo;: 1,
      &ldquo;target_column_1&rdquo;: 2
    },
    {
      &ldquo;type&rdquo;: &ldquo;remove_columns&rdquo;,
      &ldquo;target_columns&rdquo;: [
        1,
        2
      ]
    }
  ],
  &ldquo;mapping&rdquo;: [
    {
      &ldquo;type&rdquo;: &ldquo;list_identifiers&rdquo;,
      &ldquo;columns&rdquo;: [
        0
      ],
      &ldquo;editing&rdquo;: false
    },
    {
      &ldquo;type&rdquo;: &ldquo;url&rdquo;,
      &ldquo;columns&rdquo;: [
        1
      ]
    }
  ],
  &ldquo;extension&rdquo;: &ldquo;fastqsanger.gz&rdquo;
}
</tt>
  </div>
</div></p>

<p>Once the collection is created and uploaded, you will see a new green box in the history pane as shown in Fig. <a href="#figure-2">2</a> below:</p>

<hr>

<p><img src="/lab_site/images/hiv_collection.png" alt="" /></p>

<h5 id="figure-2">Figure 2</h5>

<p>HIV datasets uploaded as a collection. In this case the collection contains four datasets</p>

<hr>

<h1 id="quality-control">Quality control</h1>

<h2 id="assessing-read-quality">Assessing read quality</h2>

<p>Once the data is we will need to assess its quality.</p>

<h4 id="run-fastqc-with-the-following-parameters">Run <strong>FastQC</strong> with the following parameters:</h4>

<ul>
<li><em>&ldquo;Short read data from your current history&rdquo;</em>: <code>Jair data</code> (or whatever other name you gave that collection; red arrow in Fig. <a href="#figure-3">3</a>)</li>
<li>Click <strong>Execute</strong> button</li>
</ul>

<div class="alert alert-warning" role="alert">
Note that <b>FastQC</b> produces two output collections: (1) "Webpage" and (2) "Raw Data".
</div>

<hr>

<p><img src="/lab_site/images/hiv_fastqc1.png" alt="Fastqc interface" /></p>

<h5 id="figure-3">Figure 3</h5>

<p>FastqQC using collection as an input. Note that a collections button  was pressed to reveal the collection in the history</p>

<hr>

<p><strong>FastQC</strong> produces an individual report for each dataset. This means that in order to get an idea about quality of the data one needs to click report corresponding to each element of the collection. This may be possible for four datasets but will be very difficult for more than that. Fortunately, there is a tool, <strong>MultiQC</strong> that summarize data for all datasets at once. It takes collection produced by <strong>FastQC</strong> as input and produces a single report. Let apply it our example:</p>

<h4 id="summarizing-fastqc-results">Summarizing FastQC results</h4>

<p>Riun <strong>MultiQC</strong> {% icon tool %} with the following parameters:
 - <em>&ldquo;Which tool was used generate logs?&rdquo;</em> : <code>FastQC</code> (red arrow in  Fig. <a href="#figure-4">4</a>)
 - <em>&ldquo;FastQC output&rdquo;</em>: <code>FastQC on collection 1: Raw data</code>; blue arrow in Fig. <a href="#figure-4">4</a>)
 - Click <strong>Execute</strong> button</p>

<hr>

<p><img src="/lab_site/images/hiv_multiqc1.png" alt="MultiQC interface" /></p>

<h5 id="figure-4">Figure 4</h5>

<p>Running MultiQC on FastQC output. Because FastQC was run on a collection, it produced collections as outputs. Here one of these collections, &lsquo;Raw data&rsquo;, is used as an input</p>

<hr>

<p>MultiQC produces a variety of graphical summaries including distribution of quality values across reads shown in Fig. 5:</p>

<p><img src="/lab_site/images/hiv_multiqc2.png" alt="MultiQC report" /></p>

<h5 id="figure-5">Figure 5</h5>

<p>Distribution of quality values across four datasets in our example.</p>

<hr> 

<h2 id="trimming-adapters">Trimming adapters</h2>

<p>The original data produced by {% cite Jair2019-hw %} was contaminated with <a href="https://www.nature.com/protocolexchange/system/uploads/6661/original/SupplementaryDocument2-illumina-adapter-sequences-Feb2018.pdf?1530635414">Nextera adapter sequences</a>. We removed these adapters during downsampling (because they interfered with mapping). However, for the same of this presentation, let assume that our reads are in fact contaminated with adapters.</p>

<p>A great tool for automatic detection and removal of adapters in <strong>Trim Galore!</strong>, which, in turn, is based on another widely used tools called <strong>CutAdapt</strong>. Let use <strong>trim-galore!</strong> to remove adapters:</p>

<h4 id="trimming-adapters-1">Trimming adapters</h4>

<p>Run <strong>Trim Galore!</strong> {% icon tool %} with the following parameters:
 - <em>&ldquo;Is this library paired- or single-end?&rdquo;</em> : <code>Single-end</code> (green arrow in Fig. <a href="#figure-6">6</a>)
 - <em>&ldquo;Reads in FASTQ format&rdquo;</em> : <code>Jair data</code> (or whatever other name you gave that collection; grey arrow in Fig. <a href="#figure-6">6</a>
 - <em>&ldquo;Trim Galore! advanced settings&rdquo;</em> : <code>Full parameter list</code> (blue arrow in Fig. <a href="#figure-6">6</a>)
 - <em>&ldquo;Discard reads that became shorter than length N&rdquo;</em> : <code>0</code> (red arrow in Fig. <a href="#figure-6">6</a>)
 - Click <strong>Execute</strong> button</p>

<div class="alert alert-warning" role="alert">
<b>Why do we use `Single-end` setting while our data is actually paired-end?</b><br><hr> Our data is in interleaved paired-end format. It is a single file that contains forward and reverse reads in an alternating order. Here we are fooling <b>Trim Galore!</b> into processing this file as a single end. In order to do this we set <em>"Discard reads that became shorter than length N"</em> to <tt>0</tt>. This guarantees that no reads will be thrown away and the alternating order of forward and reverse reads in our file will not be disrupted. 
</div>

<div class="alert alert-info" role="alert">

<b>Tip: What did Trim Galore! do?</b><hr>
If you scroll down interface of the <b>Trim Galore!</b> tool, you will see <em>"Generate a report file"</em> selector. Setting this to <tt>Yes</tt> will produce a report dataset that can be processed with <b>MultiQC</b>. This will provide information on what types and how many adapters have been removed from the reads. When running <b>MultiQC</b> simply set <em>"Which tool was used generate logs?"</em> to <tt>CutAdapt / Trim Galore!</tt>.
</div>

<hr>

<p><img src="/lab_site/images/hiv_tg1.png" alt="Trim Galore interface" /></p>

<h5 id="figure-6">Figure 6</h5>

<p>Trim Galore! interface. Note that <em>Discard reads that became shorter than length N</em> is set to <tt>0</tt>(red arrow)</p>

<hr>

<h1 id="mapping-and-estimating-coverage">Mapping and estimating coverage</h1>

<h2 id="get-the-hiv-genome">Get the HIV genome</h2>

<p>Now it is time to map the reads against the HIV reference. We need to download that reference first. The sequence can be obtained directly from <a href="https://www.ncbi.nlm.nih.gov/">NCBI</a> using the following URL:</p>

<pre><code>https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=nucleotide&amp;id=K03455.1&amp;rettype=fasta
</code></pre>

<p>This URL can be pasted directly into <strong>Upload</strong> utility:</p>

<h4 id="upload-with-upload-utility">Upload with Upload Utility</h4>

<ol>
<li>Click the upload icon {% icon galaxy-upload %} in the upper left corner of the Galaxy interface</li>
<li>Select <strong>Regular</strong> tab</li>
<li>Click <strong>Paste/Fetch data</strong> button</li>
<li>A text box <em>&ldquo;Download data from the web by entering URLs (one per line) or directly paste content&rdquo;</em> will appear</li>
<li>Paste the URL we just mentioned into this box</li>
<li>Set <strong>Type</strong> to <code>fasta</code></li>
<li>Click <strong>Start</strong></li>
<li>A dataset with a long ugly name will appear in history</li>
<li>Rename dataset as <code>hxb2</code> (click the pencil icon)</li>
</ol>

<h2 id="map-against-hiv-genome-using-bwa-mem">Map against HIV genome using <strong>BWA-MEM</strong></h2>

<p>Next, we will map reads trimmed at the previous step against the newly downloaded genome, which just appeared as a new history item:</p>

<h4 id="mapping-with-bwa-mem">Mapping with BWA-MEM</h4>

<p>Use <strong>BWA-MEM</strong> with the following parameters:</p>

<ol>
<li><em>&ldquo;Will you select a reference genome from your history or use a built-in index?&rdquo;</em> : <code>Use a genome from history and build index</code> (red arrow in Fig. <a href="#figure-7">7</a>)</li>
<li><em>&ldquo;Use the following dataset as the reference sequence&rdquo;</em> : <code>hxb2</code> or however you named just uploaded HIV genome (blue arrow in Fig. <a href="#figure-7">7</a>)</li>
<li><em>&ldquo;Single or Paired-end reads&rdquo;</em> : <code>Paired Interleaved</code> (green arrow in Fig. <a href="#figure-7">7</a>)</li>
<li><em>&ldquo;Select fastq dataset&rdquo;</em> : Click {% icon param-collection %} and select collection produced by <strong>Trim Galore!</strong> (orange arrow in Fig. <a href="#figure-7">7</a>)</li>
<li><em>&ldquo;Set read groups information?&rdquo;</em> : <code>Set read groups (SAM/BAM specification)</code></li>
<li>Click <strong>Execute</strong></li>
</ol>

<hr>

<p><img src="/lab_site/images/hiv_bwa1.png" alt="bwa mem interface" /></p>

<h5 id="figure-7">Figure 7</h5>

<p><strong>BWA MEM</strong> interface
</hr></p>

<h2 id="estimate-coverage">Estimate coverage</h2>

<p>To proceed with variant calling we need to estimate coverage or reads across the HIV genome. The first step in this analysis is to compute coverage using <strong>bedtools Genome Coverage</strong> tool:</p>

<h4 id="calculating-coverage-with-bedtools">Calculating coverage with BEDtools</h4>

<p>Use <strong>bedtools Genome Coverage</strong> {% icon tool %} with the following parameters:</p>

<ol>
<li><em>&ldquo;Input type&rdquo;</em> : Set to <code>BAM</code> (red arrow in Fig. <a href="#figure-8">8</a>)</li>
<li><em>&ldquo;BAM file&rdquo;</em> : Click {% icon param-collection %} and select collection produced by <strong>BWA MEM</strong> during the previous step (blue arrow in Fig. <a href="#figure-8">8</a>)</li>
<li>Click <strong>Execute</strong></li>
</ol>

<hr>

<p><img src="/lab_site/images/hiv_cvrg1.png" alt="bamtools genome coverage interface" /></p>

<h5 id="figure-8">Figure 8</h5>

<p><strong>Bedtools Genome Coverage</strong> interface.
<hr></p>

<p><strong>bedtools Genome Coverage</strong> will produce output that would look like this</p>

<pre><code>K03455.1	2016	2017	185
K03455.1	2017	2019	187
K03455.1	2019	2020	188
K03455.1	2020	2022	186
K03455.1	2022	2023	187
K03455.1	2023	2026	188
</code></pre>

<p>where the first column is the genome id, second and third are start and end of a bin, respectively, and the last one is coverage. Such data is produced for all four datasets in our collection. We need to aggregate these data by computing mean and median across the four datasets. This can be done with <strong>Datamash</strong> tool:</p>

<h4 id="computing-mean-and-median-with-datamash">Computing mean and median with Datamash</h4>

<p>Use <strong>Datamash</strong> {% icon tool %} with the following parameters:</p>

<ol>
<li><em>&ldquo;Input tabular dataset&rdquo;</em> : Click {% icon param-collection %} and select collection produced by <strong>bedtools Genome Coverage</strong> during the previous step (red arrow in Fig. <a href="#figure-9">9</a>)</li>
<li><em>&ldquo;Operation to perform on each group&rdquo;</em> : set <strong>Type</strong> to <code>mean</code> and <strong>On column</strong> to <code>4</code> (blue arrow in Fig. <a href="#figure-9">9</a>)</li>
<li>Click <em>&ldquo;Insert operation to perform on each group</em>&rdquo; (orange arrow in Fig. <a href="#figure-9">9</a>)</li>
<li>Repeat step 2 with one exception: set <strong>Type</strong> to <code>median</code> (green arrow in Fig. <a href="#figure-9">9</a>)</li>
<li>Click <strong>Execute</strong></li>
</ol>

<hr>

<p><img src="/lab_site/images/hiv_datamash1.png" alt="datamash interface" /></p>

<h5 id="figure-9">Figure 9</h5>

<p><strong>Datamash</strong> interface
<hr></p>

<p>for each dataset <strong>Datamash</strong> will produce two numbers like this:</p>

<pre><code>918.05347593583	880
</code></pre>

<p>where the first value is mean and the last is median.</p>

<h2 id="collapsing-collection-into-a-single-dataset">Collapsing collection into a single dataset</h2>

<p>Now we want to aggregate these into a single dataset and generate a histogram. To do this we will use a special class of Galaxy tools called collection operations. Specifically, we will use <strong>Collapse Collection</strong> tool:</p>

<h4 id="collapsing-a-dataset-collection">Collapsing a dataset collection</h4>

<p>Use <strong>Collapse Collection</strong> {% icon tool %} with the following parameters:</p>

<ol>
<li><em>&ldquo;Collection of files to collapse into single dataset&rdquo;</em> : Click collection button (folder icon) and select collection produced by <strong>Datamash</strong> during the previous step (red arrow in Fig. <a href="#figure-10">10</a>)</li>
<li><em>&ldquo;Prepend File name&rdquo;</em> :<code>Yes</code> (blue arrow in Fig. <a href="#figure-10">10</a>)</li>
<li><em>&ldquo;Where to add dataset name</em>&rdquo; : <code>Same line and each line in dataset</code> (green arrow in Fig. <a href="#figure-10">10</a>)</li>
</ol>

<hr>

<p><img src="/lab_site/images/hiv_collapse1.png" alt="collapse collection" /></p>

<h5 id="figure-10">Figure 10</h5>

<p>Collapse collection interface
<hr></p>

<p><strong>Collapse Collection</strong> will produce the following output:</p>

<pre><code>SRR8525909	918.05347593583	880
SRR8525905	269.6483909416	274
SRR8525896	85.444444444444	4
SRR8525889	11922.120564344	12696
</code></pre>

<p>you can see that selecting options highlighted with blue and green arrows (see Fig. 10) instructed <strong>Collapse Collection</strong> to pre-pend dataset names to each corresponding line. We can now use these data to draw a simple barplot using Galaxy&rsquo;s built-in visualization engine:</p>

<h4 id="building-a-histogram">Building a histogram</h4>

<ol>
<li>Locate <strong>Visualize</strong> at the very top of Galaxy interface</li>
<li>Click and select <strong>Create visualization</strong></li>
<li>Click on <strong>Bar Horizontal (NVD3)</strong></li>
<li>A dropdown <strong>Select a dataset to visualize:</strong> will appear. From this dropdown select the result of <strong>Collapse Collection</strong> step (red arrow in Fig. <a href="#figure-11">11</a>)</li>
<li>A bar chart will appear. Use disk icon to modify chart parameters.</li>
<li>Change <strong>Data point labels</strong> to <code>Column 1</code></li>
<li>Change <strong>Values for x-axis</strong> to <code>Column 1</code></li>
</ol>

<hr>

<p><img src="/lab_site/images/hiv_chart1.png" alt="Selecting dataset for bar chart" /></p>

<h5 id="figure-11">Figure 11</h5>

<p>Selecting dataset to produce bar chart.
<hr></p>

<p>The resulting chart will look like this:</p>

<p><img src="/lab_site/images/hiv_chart2.png" alt="Selecting dataset for bar chart" /></p>

<h5 id="figure-12">Figure 12</h5>

<p>Selecting dataset to produce bar chart.</p>

<p>Here you can see that coverage varies dramatically between datasets.</p>

<h1 id="dynamic-downsampling">Dynamic downsampling</h1>

<p>The chart above shown that one of the datasets, <code>SRR8525889</code>, has a very high coverage. Normally this should not be a problem. However, because we will performing variant calling using <strong>Freebayes</strong> this may cause problems. Specifically, <strong>Freebayes</strong> tends to &ldquo;stall&rdquo; on the datasets that have high coverage variation - it never finished. To alleviate this problem we can simply downsample the BAM files to approximately 1,000 $$\times$$ coverage.</p>

<p>Yet there is one difficulty. If we were dealing with a single dataset, that would be easy - just run a downsampling tools (such as <strong>DownsampleSam</strong> from the Picard package). But we have multiple datasets. These datasets also have diffent coverage, so each needs to be downsampled differently. Thus we need some way for doing this automatically (just imagine having hundreds or trousands of datasets).</p>

<p>Galaxy has a new functionality for addressing problems of this kind. This functionality is called <em>expression tools</em>. (To learn more about expression tools see the following <a href="https://training.galaxyproject.org/training-material/topics/galaxy-ui/tutorials/workflow-parameters/tutorial.html">tutorial</a>.</p>

<p>The basic idea is this:</p>

<ul>
<li>we compute the mean coverage for each dataset</li>
<li>we feed this value to a special <strong>expression tool</strong></li>
<li>this <strong>expression tool</strong> puts this value (or <em>expression</em>) as a <strong>parameter</strong> to downsampling tool</li>
<li>because this value will be calculated for each dataset individually, they will be downsampled dynamically: each according to its own coverage mean.</li>
</ul>

<div class="alert alert-info" role="alert">
<b>The point of expression tools</b><hr>
Normal Galaxy tools take files as inputs and produce files as outputs. The expression tools take files as inputs but instead of producing files as outputs they output parameters that can be used in other tools.
</div>

<p>Let&rsquo;s do this by example. Look at the following workflow in Fig. 13:</p>

<hr>

<p><img src="/lab_site/images/hiv_ds_workflow1.png" alt="Downsampling workflow" /></p>

<h5 id="figure-13">Figure 13</h5>

<p>Downsampling workflow from 10,000 feet.
<hr></p>

<p>It has takes two inputs (&ldquo;BAMs&rdquo; and &ldquo;Datamash results&rdquo;) and four steps. Before discussing this workflow in details let&rsquo;s describe the overall logic.</p>

<h2 id="the-logic">The logic</h2>

<p>For dataset <code>SRR8525902</code> mean and median of coverage look like this:</p>

<pre><code>8407.4972138877	7140
</code></pre>

<p>To downsample a dataset with mean coverage of $$8407$$ to about $$1000\times$$ we need to downsample it by:</p>

<p>$$ \frac{1000}{8407} \approx 0.11 $$</p>

<p>the means that we need to keep approximately 11% of the original reads. At the same time other datasets in our example have relatively low coverage. For example <code>SRR8525905</code> has the following stats:</p>

<pre><code>269.6483909416	274
</code></pre>

<p>for this dataset we do not need to do anything. However, because we will be computing expression</p>

<p>$$ \frac{1000}{\bar{C}} $$</p>

<p>(where C is the mean coverage) for all samples for <code>SRR8525905</code> we will get ~ 3.7, which is larger than 1 and therefore impossible. To avoid such situations we will be taking a $$min(\frac{1000}{\bar{C}},1)$$.</p>

<h2 id="workflow-inputs">Workflow Inputs</h2>

<ol>
<li>The first input in BAM datasets. These, in the case of our tutorial, are generated with <strong>BWA-MEM</strong>. It is just a collection of BAM datasets. It is exactly what we&rsquo;ve produced at <a href="#map-against-hiv-genome-using-bwa-mem">mapping step</a>.</li>
<li>The second input is <strong>Datamash</strong> output produced by us at <a href="#estimate-coverage">estimate coverage</a> step.</li>
</ol>

<h2 id="workflow-steps">Workflow Steps</h2>

<h3 id="compute">Compute</h3>

<p>The first step, <code>Compute</code> takes input of <strong>Datamash</strong>. Again, it looks like this:</p>

<pre><code>269.6483909416	274
</code></pre>

<p>The <strong>Compute</strong> tool is designed to perform simple computations on tab delimited files. In this case we will set <em>&ldquo;Add expression&rdquo;</em> parameter if this tool to <code>min(1000/c1,1)</code>. This means that the tools will compute the value of 1,000 divided by the content of the first column <code>c1</code>. If this value is less than 1, it will add a column to the input dataset containing this value. If the values is greater than 1, it will add <code>1</code> as a new column to the input dataset. The following table contains an example of such calculation for two datasets:</p>

<table>
<thead>
<tr>
<th></th>
<th align="center">SRR8525905</th>
<th align="center">SRR8525889</th>
</tr>
</thead>

<tbody>
<tr>
<td>Input to <strong>Compute</strong> tool</td>
<td align="center"><code>269.64 274</code></td>
<td align="center"><code>11922.12 12696</code></td>
</tr>

<tr>
<td>Output of <strong>Compute</strong> tool</td>
<td align="center"><code>269.64 274 1</code></td>
<td align="center"><code>11922.12 12696 0.083</code></td>
</tr>
</tbody>
</table>

<h3 id="cut">Cut</h3>

<p>The step cuts out the column produced by <strong>Compute</strong> tool from its output. This step is executed using <strong>Cut</strong> tools. To illustrate what is happening let&rsquo;s use the table from previous step:</p>

<table>
<thead>
<tr>
<th></th>
<th align="center">SRR8525905</th>
<th align="center">SRR8525889</th>
</tr>
</thead>

<tbody>
<tr>
<td>Input to <strong>Cut</strong> tool</td>
<td align="center"><code>269.64 274 1</code></td>
<td align="center"><code>11922.12 12696 0.083</code></td>
</tr>

<tr>
<td>Output of <strong>Cut</strong> tool</td>
<td align="center"><code>1</code></td>
<td align="center"><code>0.083</code></td>
</tr>
</tbody>
</table>

<h3 id="parse-parameter-value">Parse parameter value</h3>

<p>This is the <strong>Expression tool</strong> step. In Fig. <a href="#figure-13">13</a> you can see that the <em>&ldquo;Float param&rdquo;</em> output of <strong>Parse parameter value</strong> expression tool is connected with <em>&ldquo;Probability&rdquo;</em> parameter of <strong>Downsample SAM/BAM</strong> tool. Thus it supplies this tool with the <em>&ldquo;Probability&rdquo;</em> value appropriate for a given dataset. Thus this tools get different probability for each input dataset.</p>

<h3 id="downsample-sam-bam">Downsample SAM/BAM</h3>

<p>This step randomly samples reads from the BAM value according to supplied <em>&ldquo;Probability&rdquo;</em> value. If it is, for example, <code>0.083</code> it will sample approximately 8% of the reads. If it is <code>1</code> is will sample all the reads.</p>

<h2 id="recreating-workflow">Recreating workflow</h2>

<p>To run the workflow you can either construct it from scratch or import prebuilt copy from the following URL:</p>

<pre><code>https://workshop.usegalaxy.org/u/anton/w/downsample
</code></pre>

<p>Let&rsquo;s create workflow from scratch as shown here:</p>

<p><img src="/lab_site/images/create_ds_workflow.gif" alt="Downsampling workflow" /></p>

<h5 id="figure-15">Figure 15</h5>

<p>Creating downsampling workflow with expression tools. It extremely important to save your workflow after you have built it by clicking on the save (disk) icon.</p>

<h2 id="running-workflow">Running workflow</h2>

<p>Once the workflow is created we can run it on the collection that was produced by the running <strong>BWA-MEM</strong> on HIV-1 genome.</p>

<h4 id="running-downsampling-workflow">Running downsampling workflow</h4>

<ol>
<li>Locate <strong>Workflow</strong> at the very top of Galaxy interface</li>
<li>Select the workflow you&rsquo;ve just created (it is named <code>ds</code> in Fig. <a href="#figure-14">14</a>, but you could have named it any way you want).</li>
<li>Click on the down arrow adjacent to the workflow name and select <strong>Run</strong>.</li>
<li>In the interface that would appear you need to change three things:

<ul>
<li><em>&ldquo;BAMs&rdquo;</em> : set to the output of <strong>BWA-MEM</strong> from the <a href="#map-against-hiv-genome-using-bwa-mem">mapping step</a> (red arrow in Fig. <a href="#figure-15">15</a>)</li>
<li><em>&ldquo;Datamash&rdquo;</em> : set to the [output of <strong>Datamash</strong>]](#hands_on-computing-mean-and-median-with-datamash) (blue arrow in Fig. <a href="#figure-15">15</a>)</li>
<li><em>&ldquo;Add expression&rdquo;</em> : set to <code>min(1000/c1,1)</code>. This expression will output minimal of two values: <code>1000/c1</code> or <code>1</code> (orange arrow in Fig. <a href="#figure-15">15</a>)</li>
</ul></li>
<li>Click <strong>Run workflow</strong></li>
</ol>

<p><img src="/lab_site/images/hiv_run_wf.png" alt="Running downsampling workflow" /></p>

<h5 id="figure-15-1">Figure 15</h5>

<p>Running downsampling workflow</p>

<h1 id="calling-variants">Calling variants</h1>

<p>We are now ready to identify sequence variants in our data. For this purpose we will use <strong>Freebayes</strong> - a versatile variant calling suitable for mixed haploid samples.</p>

<h4 id="calling-variants-with-freebayes">Calling variants with Freebayes</h4>

<ol>
<li><em>&ldquo;Choose the source for the reference genome&rdquo;</em> : <code>History</code> (red arrow in Fig. <a href="#figure-16">16</a>)</li>
<li><em>&ldquo;BAM dataset&rdquo;</em> : Set this to the collection produced by the downsampling workflow (blue arrow in Fig. <a href="#figure-16">16</a>)</li>
<li><em>&ldquo;Use the following dataset as the reference sequence&rdquo;</em> : Set this to HIV genome sequence <a href="#get-the-hiv-genome">uploaded previously</a> (orange arrow in Fig. <a href="#figure-16">16</a>)</li>
<li><em>&ldquo;Choose parameter selection level&rdquo;</em> : <code>Full list of options</code></li>
<li>Locate <em>&ldquo;Population model options&rdquo;</em> dropdown and set it to <code>Set population model options</code>. Within this section set the following:

<ul>
<li><em>&ldquo;Set ploidy for the analysis&rdquo;</em> : <code>1</code> (red arrow in Fig. <a href="#figure-17">17</a>)</li>
<li><em>&ldquo;Output all alleles which pass input filters, regardless of genotyping outcome or model&rdquo;</em> : <code>Yes</code> (blue arrow in Fig. <a href="#figure-17">17</a>)</li>
</ul></li>
<li>Locate <em>&ldquo;Input filters&rdquo;</em> dropdown and set it to <code>Set input filters</code>. Within this section set the following:

<ul>
<li><em>&ldquo;Use stringent input base and mapping quality filters&rdquo;</em> : <code>Yes</code> (red arrow in Fig. <a href="#figure-18">18</a>)</li>
<li><em>&ldquo;Require at least this fraction of observations supporting an alternate allele within a single individual in the in order to evaluate the position&rdquo;</em> : <code>0.001</code> (blue arrow in Fig. <a href="#figure-18">18</a>)</li>
<li><em>&ldquo;Require at least this count of observations supporting an alternate allele within a single individual in order to evaluate the position&rdquo;</em> : <code>10</code> (orange arrow in Fig. <a href="#figure-18">18</a>)</li>
</ul></li>
<li>Click <strong>Execute</strong></li>
</ol>

<p><img src="/lab_site/images/hiv_freebayes1.png" alt="Freebayes datasets" /></p>

<h5 id="figure-16">Figure 16</h5>

<p>Setting inputs for Freebayes
<img src="/lab_site/images/hiv_freebayes2.png" alt="Freebayes population settings" /></p>

<h5 id="figure-17">Figure 17</h5>

<p>Setting population options
<img src="/lab_site/images/hiv_freebayes3.png" alt="Freebayes input filters" /></p>

<h5 id="figure-18">Figure 18</h5>

<p>Setting input filters</p>



      </main>
    </div>
    
<footer id="footer" class="mt-auto text-center text-muted">
  <div class="container">
    AN LAB | CC-BY
  </div>
</footer>

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"
    onload="renderMathInElement(document.body);"></script>


    <script src="https://nekrut.github.io/lab_site/js/feather.min.js"></script>
<script>
  feather.replace()
</script>


<script src="https://nekrut.github.io/lab_site/js/jquery-3.3.1.slim.min.js"></script>
<script src="https://nekrut.github.io/lab_site/js/bootstrap.bundle.min.js"></script>

    
  
  <script>
  window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
  ga('create', 'UA-123456789-1', 'auto');
  ga('send', 'pageview');
  </script>
  <script async src='https://www.google-analytics.com/analytics.js'></script>
  

  </body>
</html>